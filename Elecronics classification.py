# -*- coding: utf-8 -*-
"""MS2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NB4ZF_S6frdwX1UuGJPohbQMYnJQzHcr
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.feature_selection import SelectKBest
from sklearn import metrics
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from scipy.stats import chi2_contingency
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_classif
from sklearn.impute import SimpleImputer
import pickle
import os

for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

"""# **Read file**"""

dataset= pd.read_csv("ElecDeviceRatingPrediction_Milestone2.csv")
dataset

"""# **Preprocessing**

1. Handling Duplicates and Missing Data (DONE)
"""

duplicate_rows = dataset.duplicated()
duplicate_rows.sum()

dataset.drop_duplicates(inplace=True)
duplicate_rows = dataset.duplicated()
duplicate_rows.sum()

null_count = (dataset.processor_gnrtn == 'Not Available').sum()
print(null_count)

columns_with_missing_values = ["processor_gnrtn"]
mode_value = dataset['processor_gnrtn'].mode()[0]
dataset['processor_gnrtn'] = dataset['processor_gnrtn'].replace('Not Available', mode_value)
print(dataset["processor_gnrtn"])

numerical_features=['Price','Number of Ratings','Number of Reviews']

categorical_features = ["brand", "processor_brand", "processor_name", "ram_type", "weight",
                        "Touchscreen", "msoffice", "os", "processor_gnrtn","rating",'warranty','hdd','ssd','graphic_card_gb','ram_gb']

column_mode = dataset[categorical_features].mode()
dataset[categorical_features] = dataset[categorical_features].fillna(column_mode)

column_mean = dataset[numerical_features].mean()
dataset[numerical_features] = dataset[numerical_features].fillna(column_mean)

"""### **3. Feature Scaling**"""

scaler = MinMaxScaler()
dataset[numerical_features] = scaler.fit_transform(dataset[numerical_features])
with open('scaling.pkl', 'wb') as file:
    pickle.dump(scaler, file)

train_feature_names = dataset[numerical_features].columns

"""# **4. Encoding Categorical Data**"""

ordinal_features = ["processor_gnrtn","rating","warranty","ram_gb", "ssd","hdd","graphic_card_gb"]
nominal_features = ["brand", "processor_brand", "processor_name", "ram_type", "weight", "Touchscreen", "msoffice","os"]

def encode_nominal_features(dataset, nominal_features):
    encoders = {}
    categorical_features = []
    for column in nominal_features:
        one_hot_encoder = OneHotEncoder()
        transformed_data = one_hot_encoder.fit_transform(dataset[[column]]).toarray()
        new_columns = one_hot_encoder.get_feature_names_out([column])
        categorical_features.extend(new_columns)
        df_encoded = pd.DataFrame(transformed_data, columns=new_columns)
        for col in df_encoded.columns:
            dataset[col] = df_encoded[col].values
        encoders[column] = one_hot_encoder

    return dataset, encoders, categorical_features
dataset, encoders, categorical_features = encode_nominal_features(dataset, nominal_features)

with open('one_hot_encoders.pkl', 'wb') as file:
    pickle.dump(encoders, file)

encoders

label_encoders = {}

for column in ordinal_features:
    label_encoder = LabelEncoder()
    dataset[column] = label_encoder.fit_transform(dataset[column])
    label_encoders[column] = label_encoder

with open('label_encoders.pkl', 'wb') as file:
     pickle.dump(label_encoders, file)

"""# **5. Features Selection**

# **chi square**
"""

categorical_columns =[categorical_features]

significant_features = []
for column in categorical_features:

        contingency_table = pd.crosstab(dataset[column], dataset['rating'])
        chi2, p, dof, expected = chi2_contingency(contingency_table)
        if p < 0.01:
            significant_features.append(column)

selected_features_chiSquare = dataset[significant_features]
with open('selector chi_square.pkl', 'wb') as file:
    pickle.dump(selected_features_chiSquare, file)


print(selected_features_chiSquare)

"""info

# **Anova**
"""

selector = SelectKBest(score_func=f_classif, k='all')

selector.fit(dataset[numerical_features], dataset['rating'])

scores = -np.log10(selector.pvalues_)


feature_scores = pd.DataFrame(list(zip(dataset[numerical_features], scores)), columns=['Feature', 'Score'])

selected_features_Anova = feature_scores[feature_scores['Score'] > 0.5]['Feature']
selected_features_Anova = dataset[selected_features_Anova.tolist()]
with open('selector Anova.pkl', 'wb') as file:
    pickle.dump(selected_features_Anova, file)

print(selected_features_Anova)

combined_features = list(set( selected_features_chiSquare + selected_features_Anova))

x_data = dataset[combined_features]
y_data = dataset['rating']

print(x_data)

"""# **6. Data Spiltting**"""

X_train, X_test, y_train, y_test = train_test_split(x_data , y_data, test_size = 0.20, shuffle = True, random_state = 42, stratify=y_data)

training_time=[]
test_time=[]
models_Acuracy =[]

np.random.seed(42)

"""# **Decision Tree**"""

import pickle
import time
clf = DecisionTreeClassifier(min_samples_leaf = 10 ,max_depth = 100, max_features = 'log2' )

start_train = time.time()
clf.fit(X_train, y_train)
end_train = time.time()
training_tim = end_train - start_train
training_time.append(training_tim)

with open('decision_tree_model.pkl', 'wb') as file:
    pickle.dump(clf, file)


start_test = time.time()
y_pred_test = clf.predict(X_test)
end_test = time.time()
test_tim = end_test - start_test
test_time.append(test_tim)


y_pred_train = clf.predict(X_train)

accuracy_test = accuracy_score(y_test, y_pred_test) * 100
models_Acuracy.append(accuracy_test)

accuracy_train = accuracy_score(y_train, y_pred_train) * 100
print("Accuracy for test:", accuracy_test)
print("Accuracy for train:", accuracy_train)

""" # **Random Forest**"""

rf_clf = RandomForestClassifier(n_estimators = 150 , min_samples_leaf =15 , max_depth = 5)
rf_clf.fit(X_train, y_train)


with open('random_forest_model.pkl', 'wb') as file:
    pickle.dump(rf_clf, file)
start_train = time.time()
rf_clf.fit(X_train, y_train)
end_train = time.time()
training_tim = end_train - start_train
training_time.append(training_tim)


start_test = time.time()
y_pred_test = rf_clf.predict(X_test)
end_test = time.time()
test_tim = end_test - start_test
test_time.append(test_tim)


y_pred_train = rf_clf.predict(X_train)

accuracy_test = accuracy_score(y_test, y_pred_test) * 100
models_Acuracy.append(accuracy_test)
accuracy_train = accuracy_score(y_train, y_pred_train) * 100
print("Accuracy for test:", accuracy_test)
print("Accuracy for train:", accuracy_train)

"""# **Support Vector Machine (SVM)**"""

svm_clf = SVC(kernel='poly', C = 100 ,gamma = 10 )



start_train = time.time()
svm_clf.fit(X_train, y_train)
end_train = time.time()
training_tim = end_train - start_train
training_time.append(training_tim)

with open('svm_model.pkl', 'wb') as file:
    pickle.dump(svm_clf, file)

start_test = time.time()
y_pred_test = svm_clf.predict(X_test)
end_test = time.time()
test_tim = end_test - start_test
test_time.append(test_tim)


y_pred_train = svm_clf.predict(X_train)
accuracy_test = accuracy_score(y_test, y_pred_test) * 100
models_Acuracy.append(accuracy_test)
accuracy_train = accuracy_score(y_train, y_pred_train) * 100
print("Accuracy for test:", accuracy_test)
print("Accuracy for train:", accuracy_train)

"""#  **K-Nearest Neighbors (KNN)**"""

knn_clf = KNeighborsClassifier(algorithm= 'kd_tree', weights ='uniform',n_neighbors = 6)


start_train = time.time()
knn_clf.fit(X_train, y_train)
end_train = time.time()
training_tim = end_train - start_train
training_time.append(training_tim)

with open('knn_model.pkl', 'wb') as file:
    pickle.dump(knn_clf, file)


start_test = time.time()
y_pred_test = knn_clf.predict(X_test)
end_test = time.time()
test_tim = end_test - start_test
test_time.append(test_tim)

y_pred_train = knn_clf.predict(X_train)

accuracy_test = accuracy_score(y_test, y_pred_test) * 100
models_Acuracy.append(accuracy_test)
accuracy_train = accuracy_score(y_train, y_pred_train) * 100
print("Accuracy for test:", accuracy_test)
print("Accuracy for train:", accuracy_train)

"""# **Naive Bayes**"""

from sklearn.naive_bayes import GaussianNB
gnb = GaussianNB(var_smoothing=1e-2)

start_train = time.time()
gnb.fit(X_train, y_train)
end_train = time.time()
training_tim = end_train - start_train
training_time.append(training_tim)

start_test = time.time()
y_pred_test = gnb.predict(X_test)
end_test = time.time()
test_tim = end_test - start_test
test_time.append(test_tim)

y_pred_train = gnb.predict(X_train)
accuracy_test = accuracy_score(y_test, y_pred_test) * 100
models_Acuracy.append(accuracy_test)
accuracy_train = accuracy_score(y_train, y_pred_train) * 100
print("Accuracy for test:", accuracy_test)
print("Accuracy for train:", accuracy_train)

"""#  **Logistic Regression**"""

lr_clf = LogisticRegression(C =1 ,tol = 2, penalty = None)


start_train = time.time()
lr_clf.fit(X_train, y_train)
end_train = time.time()
training_tim = end_train - start_train
training_time.append(training_tim)

with open('Logistic_Regression.pkl', 'wb') as file:
    pickle.dump(lr_clf, file)

start_test = time.time()
y_pred_test = lr_clf.predict(X_test)
end_test = time.time()
test_tim = end_test - start_test
test_time.append(test_tim)


y_pred_train = lr_clf.predict(X_train)

accuracy_test = accuracy_score(y_test, y_pred_test) * 100
models_Acuracy.append(accuracy_test)
accuracy_train = accuracy_score(y_train, y_pred_train) * 100
print("Accuracy for test:", accuracy_test)
print("Accuracy for train:", accuracy_train)

"""# **DNN**"""

import tensorflow
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.utils import to_categorical
from sklearn.metrics import roc_auc_score, roc_curve
from tensorflow.keras.optimizers import SGD
from keras.models import load_model

from tensorflow.keras.callbacks import ModelCheckpoint
output_dir = 'dense'
modelcheckpoint = ModelCheckpoint(filepath=output_dir+
                                  "/weights.{epoch:02d}.hdf5")

from keras.models import load_model


model = Sequential()
model.add(Dense(128, activation='relu', input_shape=(X_train.shape[1],)))

model.add(Dense(64, activation='relu'))

model.add(Dense(2, activation='softmax'))

model.compile(loss='sparse_categorical_crossentropy', optimizer='nadam', metrics=['accuracy'])

output_dir = 'DNN_classification_model'

if not os.path.exists(output_dir):
    os.makedirs(output_dir)

modelcheckpoint = ModelCheckpoint(output_dir + '/weights.{epoch:02d}.keras', save_weights_only=True)

npy_train = np.array(y_train)
npy_test = np.array(y_test)

epochs = 50
batch_size = 32


start_train = time.time()
model.fit(
    X_train,
    y_train,
    batch_size=batch_size,
    epochs=epochs,
    verbose=1,
    validation_data=(X_test, npy_test),
    callbacks=[modelcheckpoint]
    )
end_train = time.time()
training_tim = end_train - start_train
training_time.append(training_tim)

model.load_weights(output_dir+"/weights.46.keras")

predected_train = model.predict(X_train)
start_test = time.time()
predected_test = model.predict(X_test)
end_test = time.time()
test_tim = end_test - start_test
test_time.append(test_tim)

predected_test = np.argmax(predected_test, axis=1)
predected_train = np.argmax(predected_train, axis=1)

accuracy_test = accuracy_score(npy_test, predected_test) * 100
models_Acuracy.append(accuracy_test)
accuracy_train = accuracy_score(npy_train, predected_train) * 100
print("Accuracy for test:", accuracy_test)
print("Accuracy for train:", accuracy_train)

#print(models_Acuracy.shape)

"""# **Plot**"""

import matplotlib.pyplot as plt
model_names = ['Decision_Tree', 'Random_Forest', 'SVM','KNN','Naive_bayes','Logistic Regression', 'DNN']
plt.figure(figsize=(10, 6))
plt.bar(model_names, models_Acuracy, color='skyblue')
plt.xlabel('Models', fontsize=12)
plt.ylabel('Classification Accuracy', fontsize=12)
plt.title('Comparison of Model Accuracies', fontsize=14)

plt.show()

import matplotlib.pyplot as plt
model_names = ['Decision_Tree', 'Random_Forest', 'SVM','KNN','Naive_bayes','Logistic Regression','DNN']
plt.figure(figsize=(10, 6))
plt.bar(model_names, test_time, color='skyblue')
plt.xlabel('Models', fontsize=12)
plt.ylabel('Model Testing Time', fontsize=12)
plt.title('Comparison of Testing time', fontsize=14)

plt.show()

import matplotlib.pyplot as plt
model_names = ['Decision_Tree', 'Random_Forest', 'SVM','KNN','Naive_bayes','Logistic Regression','DNN']
plt.figure(figsize=(10, 6))
plt.bar(model_names, training_time, color='skyblue')
plt.xlabel('Models', fontsize=12)
plt.ylabel('Model Training Time', fontsize=12)
plt.title('Comparison of Training time', fontsize=14)
plt.show()

"""# **Test_Script**"""

import pandas as pd
import pickle
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import accuracy_score
test_data = pd.read_csv("ElecDeviceRatingPrediction_Milestone2.csv")

nominal_features = [ "brand","processor_brand", "processor_name", "ram_type", "weight", "Touchscreen", "msoffice","os"]
ordinal_features = ["processor_gnrtn","rating","warranty","ram_gb", "ssd","hdd","graphic_card_gb"]
numerical_features=['Price','Number of Ratings','Number of Reviews']


categorical_features = ["brand", "processor_brand", "processor_name", "ram_type", "weight",
                        "Touchscreen", "msoffice", "os", "processor_gnrtn","rating",'warranty','hdd','ssd','graphic_card_gb','ram_gb']





columns_with_missing_values = ["processor_gnrtn"]
mode_value = test_data['processor_gnrtn'].mode()[0]
test_data['processor_gnrtn'] = test_data['processor_gnrtn'].replace('Not Available', mode_value)

column_mode = test_data[categorical_features].mode()
test_data[categorical_features] = test_data[categorical_features].fillna(column_mode)
column_mean = test_data[numerical_features].mean()
test_data[numerical_features] = test_data[numerical_features].fillna(column_mean)




with open('scaling.pkl', 'rb') as file:
    loaded_scaler = pickle.load(file)


test_data[numerical_features] = loaded_scaler.transform(test_data[numerical_features])


with open('one_hot_encoders.pkl', 'rb') as file:
    loaded_encoders = pickle.load(file)


for column, one_hot_encoder in loaded_encoders.items():
    transformed_data = one_hot_encoder.transform(test_data[[column]]).toarray()
    new_columns = one_hot_encoder.get_feature_names_out([column])
    categorical_features.extend(new_columns)
    df_encoded = pd.DataFrame(transformed_data, columns=new_columns)
    for col in df_encoded.columns:
        test_data[col] = df_encoded[col].values


with open('label_encoders.pkl', 'rb') as file:
    loaded_label_encoders = pickle.load(file)
# print(loaded_label_encoders)


for column in ordinal_features:
    label_encoder = loaded_label_encoders[column]
    transformed_data = label_encoder.transform(test_data[column])

    test_data[column]=transformed_data

with open('selector chi_square.pkl', 'rb') as file:
     selected_features_chiSquare = pickle.load(file)

with open('selector Anova.pkl', 'rb') as file:
     loaded_selector = pickle.load(file)

combined_features = list(set(selected_features_chiSquare + loaded_selector))

test_data_selected = test_data[combined_features]

with open('random_forest_model.pkl', 'rb') as file:
    loaded_cl = pickle.load(file)

test_predictions_clf = loaded_cl.predict(test_data_selected)

print("Random Forest Model Accuracy for test:", accuracy_score( test_data['rating'],test_predictions_clf) * 100)


with open('decision_tree_model.pkl', 'rb') as file:
    loaded_clf = pickle.load(file)
test_predictions_clf = loaded_clf.predict(test_data_selected)

print("Decision Tree Model Accuracy for test:", accuracy_score( test_data['rating'],test_predictions_clf )*100)

