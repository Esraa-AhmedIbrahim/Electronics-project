# -*- coding: utf-8 -*-
"""MS1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1A9FcOQqcT-Af6KwJxeh6coFrUbe76Tzw
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from scipy.stats import chi2_contingency
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
from sklearn.preprocessing import PolynomialFeatures
from sklearn import metrics
from sklearn.metrics import r2_score
from sklearn.model_selection import GridSearchCV
import pickle
import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

"""# **Read Data**"""

dataset= pd.read_csv("ElecDeviceRatingPrediction.csv")
dataset

"""# **Preprocessing**

### **1. Handling Duplicates and Missing Data**  (DONE)
"""

duplicate_rows = dataset.duplicated()
duplicate_rows.sum()

dataset.drop_duplicates(inplace=True)
duplicate_rows = dataset.duplicated()
duplicate_rows.sum()

null_count = (dataset.processor_gnrtn == 'Not Available').sum()
print(null_count)

columns_with_missing_values = ["processor_gnrtn"]
mode_value = dataset['processor_gnrtn'].mode()[0]
dataset['processor_gnrtn'] = dataset['processor_gnrtn'].replace('Not Available', mode_value)
print(dataset["processor_gnrtn"])#dataset
print(null_count)

dataset['rating'] = dataset['rating'].str.extract('(\d+)').astype(int)
print(dataset['rating'])  #because rating values are "5 stars, 2 stars"

"""###**2. Data Augmentation** (Done but **MUST** check)

"""

categorical_features = ["brand", "processor_brand", "processor_name", "processor_gnrtn",
                       "ram_gb", "ram_type", "ssd", "hdd", "os","graphic_card_gb",
                       "weight", "warranty", "Touchscreen", "msoffice"]


numerical_features = ["Number of Ratings", "Number of Reviews", "Price", 'rating']

#dataset.shape
#print(dataset)

"""# **Data Augmentation**"""

#  augmented_dataset = dataset.copy()

# # for column in augmented_dataset.columns: #randomize the data
# #   augmented_dataset[column] = np.random.permutation(augmented_dataset[column])
# for column in numerical_features:
#    noise = np.random.normal(scale=10, size=len(augmented_dataset[column]))
#    augmented_dataset[column] = (augmented_dataset[column] + noise).astype(int)

# dataset = augmented_dataset

column_mode = dataset[categorical_features].mode()
dataset[categorical_features] = dataset[categorical_features].fillna(column_mode)

column_mean = dataset[numerical_features].mean()
dataset[numerical_features] = dataset[numerical_features].fillna(column_mean)

"""###**3. Encoding Categorical Data** (DONE but **MUST** check)


>**One Hot Encoder ==> Nominal Features**

>**Label Encoder ==> Ordinal Features**
"""

nominal_features = ["brand", "processor_brand", "processor_name", "ram_type", "weight", "Touchscreen", "msoffice","os"]

ordinal_features = ["processor_gnrtn", "warranty","ram_gb", "ssd","hdd","graphic_card_gb", 'rating']

"""### **4. Feature Scaling**




"""

scaler = MinMaxScaler()
dataset[numerical_features] = scaler.fit_transform(dataset[numerical_features])
with open('scaling.pkl', 'wb') as file:
    pickle.dump(scaler, file)

train_feature_names = dataset[numerical_features].columns

def encode_nominal_features(dataset, nominal_features):
    encoders = {}
    categorical_features = []
    for column in nominal_features:
        one_hot_encoder = OneHotEncoder()
        transformed_data = one_hot_encoder.fit_transform(dataset[[column]]).toarray()
        new_columns = one_hot_encoder.get_feature_names_out([column])
        categorical_features.extend(new_columns)
        df_encoded = pd.DataFrame(transformed_data, columns=new_columns)
        for col in df_encoded.columns:
            dataset[col] = df_encoded[col].values
        encoders[column] = one_hot_encoder

    return dataset, encoders, categorical_features
dataset, encoders, categorical_features = encode_nominal_features(dataset, nominal_features)

with open('one_hot_encoders.pkl', 'wb') as file:
    pickle.dump(encoders, file)

label_encoders = {}

for column in ordinal_features:
    label_encoder = LabelEncoder()
    dataset[column] = label_encoder.fit_transform(dataset[column].values)
    label_encoders[column] = label_encoder

with open('label_encoder.pkl', 'wb') as file:
     pickle.dump(label_encoders, file)

"""###**5. Features Selection**

"""

numeric_columns = dataset.select_dtypes(include=[np.number])
cor = numeric_columns.corr()

top_feature = cor.index[abs(cor['rating']) >= 0.09]
plt.subplots(figsize=(12, 8))
sns.heatmap(cor, annot=True)
plt.title('Correlation Matrix Heatmap')
plt.show()

print("Top correlated features with 'Rating':")

x_data = dataset[top_feature].drop(columns=['rating'])
y_data = dataset['rating']
print(x_data)
column_names = x_data.columns.tolist()
with open('top_correlation_matrix.pkl', 'wb') as f:
    pickle.dump(column_names, f)

"""[link text](https://)

#**SequentialFeatureSelector**  
"""

# from mlxtend.feature_selection import SequentialFeatureSelector as sfs
# from sklearn.linear_model import LinearRegression

# numeric_columns = dataset.select_dtypes(include=[np.number])


# X_data = dataset.select_dtypes(include=[np.number]).drop(columns=['rating'])
# y_data = dataset['rating'].astype('int')

# modelToTestTopFeatures = LinearRegression()
# sfs1 = sfs(modelToTestTopFeatures, k_features=10, forward=True)
# sfs1 = sfs1.fit(X_data, y_data)
# Topfeatures_names = list(sfs1.k_feature_names_)
# print(Topfeatures_names)
# x = dataset[Topfeatures_names]
# #X = X[Topfeatures_names]
# #print(X)

"""### **6. Data Spiltting**"""

X_train, X_test, y_train, y_test = train_test_split(x_data , y_data, test_size = 0.20, shuffle = True, random_state = 10, stratify=y_data)

X_train



'''pd.concat([X_train, augmented_dataset], ignore_index=True)
X_train
rating_col =dataset['rating']

df = pd.DataFrame(rating_col)
pd.concat([y_train, augmented_dataset], ignore_index=True)
y_train'''

"""# **Random Forest Regressor**"""

from sklearn.ensemble import RandomForestRegressor
from sklearn.datasets import make_regression

rf_regressor = RandomForestRegressor(n_estimators=300, random_state=42,  max_depth = 3, min_samples_split= 40, min_samples_leaf = 4, max_features = 'auto')
rf_regressor.fit(X_train, y_train)
with open('Random Forest Regressor model.pkl', 'wb') as file:
    pickle.dump(rf_regressor, file)
test_pred = rf_regressor.predict(X_test)
train_pred = rf_regressor.predict(X_train)

r2 = r2_score(y_test, test_pred)
print("R-squared:", r2*100)
mse = metrics.mean_squared_error(y_test, test_pred)
print('Test MSE:', mse)
print('Train MSE:', metrics.mean_squared_error(y_train, train_pred))



"""# **Decision Tree**"""

from sklearn.tree import DecisionTreeRegressor
dtRegressorModel = DecisionTreeRegressor(max_depth=3)
dtRegressorModel.fit(X_train, y_train)
with open('Decision Tree model.pkl', 'wb') as file:
    pickle.dump(dtRegressorModel, file)

predected_train = dtRegressorModel.predict(X_train)
predected_test = dtRegressorModel.predict(X_test)


r2 = r2_score(y_test, predected_test)
print("R-squared:", r2*100)

mse = metrics.mean_squared_error(y_test, predected_test)
print('Test MSE:', mse)
print('Train MSE:', metrics.mean_squared_error(y_train, predected_train))

"""# **XGBoost**"""

import xgboost as xgb
xg_reg = xgb.XGBRegressor(min_child_weight = 4,
                           colsample_bytree = 0.8,
                           learning_rate = 0.5,
                           max_depth = 20,
                           alpha = 10,
                           n_estimators =10,
                          subsample =1 )

xg_reg.fit(X_train, y_train)
with open('Xgboost model.pkl', 'wb') as file:
    pickle.dump(xg_reg, file)

predected_train = xg_reg.predict(X_train)
predected_test = xg_reg.predict(X_test)
r2 = r2_score(y_test, predected_test)
print("R-squared:", r2*100)
mse = metrics.mean_squared_error(y_test, predected_test)
print('Test MSE:', mse)
print('Train MSE:', metrics.mean_squared_error(y_train, predected_train))

"""# **KNN**"""

from sklearn.neighbors import KNeighborsRegressor


knn_reg = KNeighborsRegressor(algorithm='kd_tree', weights='uniform', n_neighbors=6)

knn_reg.fit(X_train, y_train)
with open('KNN model.pkl', 'wb') as file:
    pickle.dump(knn_reg, file)
y_pred_test = knn_reg.predict(X_test)
y_pred_train = knn_reg.predict(X_train)

r2 = r2_score(y_test, y_pred_test)
print("R-squared:", r2*100)

mse = metrics.mean_squared_error(y_test, y_pred_test)
print('Test MSE:', mse)
print('Train MSE:', metrics.mean_squared_error(y_train, y_pred_train))

"""# **Multiple Linear Regression**

"""

from sklearn import linear_model

mlr_xtrain =X_train
mlr_xtest =X_test
mlr_ytrain =y_train
mlr_ytest =y_test


multipleLinearRegMod = linear_model.LinearRegression()
multipleLinearRegMod.fit(X_train,y_train)
with open('Multiple Linear Regression model.pkl', 'wb') as file:
    pickle.dump(multipleLinearRegMod, file)
train_prediction= multipleLinearRegMod.predict(X_train)
test_prediction= multipleLinearRegMod.predict(X_test)

r2 = r2_score(mlr_ytest, test_prediction)
print("R-squared:", r2*100)
mse = metrics.mean_squared_error(mlr_ytest, test_prediction)
print('Test MSE:', mse)
print('Train MSE:', metrics.mean_squared_error(mlr_ytrain, train_prediction))

"""# **Ridge Regression**"""

from sklearn.linear_model import Ridge


ridge_model = Ridge(alpha=0.007)
ridge_model.fit(X_train, y_train)
with open('Ridge Regression model.pkl', 'wb') as file:
    pickle.dump(ridge_model, file)
predected_train = ridge_model.predict(X_train)
predected_test = ridge_model.predict(X_test)

r2 = r2_score(y_test, predected_test)
print("R-squared:", r2*100)
mse = metrics.mean_squared_error(y_test, predected_test)
print('Test MSE:', mse)
print('Train MSE:', metrics.mean_squared_error(y_train, predected_train))

"""# **Support Vector Regression (SVR)**"""

from sklearn.svm import SVR
from sklearn.model_selection import GridSearchCV

svr_model = SVR(C = 700000, gamma=0.0005, kernel = 'rbf')
svr_model.fit(X_train, y_train)
with open('Support Vector Regression model.pkl', 'wb') as file:
    pickle.dump(ridge_model, file)
predected_train = svr_model.predict(X_train)
predected_test = svr_model.predict(X_test)

r2 = r2_score(y_test, predected_test)
print("R-squared:", r2*100)
mse = metrics.mean_squared_error(y_test, predected_test)
print('Test MSE:', mse)
print('Train MSE:', metrics.mean_squared_error(y_train, predected_train))

"""# **DNN**"""

import tensorflow
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.utils import to_categorical
from sklearn.metrics import roc_auc_score, roc_curve
from tensorflow.keras.optimizers import SGD
from keras.models import load_model

from tensorflow.keras.callbacks import ModelCheckpoint
output_dir = 'dense'
modelcheckpoint = ModelCheckpoint(filepath=output_dir+
                                  "/weights.{epoch:02d}.hdf5")

from keras.models import load_model
model = Sequential()
model.add(Dense(128, activation='tanh', input_shape=(X_train.shape[1],)))

model.add(Dense(64, activation='linear'))

model.add(Dense(1, activation='linear'))

model.compile(loss='mean_squared_error', optimizer='rmsprop')

output_dir = 'DNN_regression_model'

if not os.path.exists(output_dir):
    os.makedirs(output_dir)

modelcheckpoint = ModelCheckpoint(output_dir + '/weights.{epoch:02d}.keras', save_weights_only=True)

npy_train = np.array(y_train)
npy_test = np.array(y_test)

epochs = 50
batch_size = 32
model.fit(
    X_train,
    npy_train,
    batch_size=batch_size,
    epochs=epochs,
    verbose=1,
    validation_data=(X_test, npy_test),
    callbacks=[modelcheckpoint]
    )

model.load_weights(output_dir+"/weights.48.keras")

predected_train = model.predict(X_train)
predected_test = model.predict(X_test)

d_test = np.argmax(predected_test, axis=1)
predected_train = np.argmax(predected_train, axis=1)

r2 = r2_score(npy_test, predected_test )
print("R-squared:", r2*100)
mse = metrics.mean_squared_error(npy_test, predected_test)
print('Test MSE:', mse)
print('Train MSE:', metrics.mean_squared_error(npy_train, predected_train))

"""# **Test Script**"""

import pandas as pd
import pickle
from sklearn.preprocessing import OneHotEncoder

test_data = pd.read_csv("ElecDeviceRatingPrediction.csv")

nominal_features = ["brand", "processor_brand", "processor_name", "ram_type", "weight", "Touchscreen", "msoffice","os"]

numerical_features = ["Number of Ratings", "Number of Reviews", "Price", 'rating']


categorical_features = ["brand", "processor_brand", "processor_name", "processor_gnrtn",
                       "ram_gb", "ram_type", "ssd", "hdd", "os","graphic_card_gb",
                       "weight", "warranty", "Touchscreen", "msoffice"]

ordinal_features = ["processor_gnrtn", "warranty","ram_gb", "ssd","hdd","graphic_card_gb", 'rating']

test_data['rating'] = test_data['rating'].str.extract('(\d+)').astype(int)



columns_with_missing_values = ["processor_gnrtn"]
mode_value = test_data['processor_gnrtn'].mode()[0]
test_data['processor_gnrtn'] = test_data['processor_gnrtn'].replace('Not Available', mode_value)


column_mode = test_data[categorical_features].mode()
test_data[categorical_features] = test_data[categorical_features].fillna(column_mode)
column_mean = test_data[numerical_features].mean()
test_data[numerical_features] = test_data[numerical_features].fillna(column_mean)



with open('scaling.pkl', 'rb') as file:
    loaded_scaler = pickle.load(file)



test_data[numerical_features] = loaded_scaler.transform(test_data[numerical_features])


with open('one_hot_encoders.pkl', 'rb') as file:
    loaded_encoders = pickle.load(file)

for column, one_hot_encoder in loaded_encoders.items():
    transformed_data = one_hot_encoder.transform(test_data[[column]]).toarray()
    new_columns = one_hot_encoder.get_feature_names_out([column])
    categorical_features.extend(new_columns)
    df_encoded = pd.DataFrame(transformed_data, columns=new_columns)
    for col in df_encoded.columns:
        test_data[col] = df_encoded[col].values


with open('label_encoder.pkl', 'rb') as file:
    loaded_label_encoder = pickle.load(file)

for column in ordinal_features:
    label_encoder = loaded_label_encoder[column]
    transformed_data = label_encoder.transform(test_data[column])
    df_encoded = pd.DataFrame(transformed_data,columns=[column])
    test_data[column]=df_encoded.values


with open('top_correlation_matrix.pkl', 'rb') as file:
     selected_features_correlation = pickle.load(file)

with open('Xgboost model.pkl', 'rb') as file:
    loaded_xg = pickle.load(file)

test_predictions_clf = loaded_xg.predict(test_data[selected_features_correlation])

print("XGBoost Model R2 for test:", r2_score(test_data['rating'], test_predictions_clf) * 100)

with open('Random Forest Regressor model.pkl', 'rb') as file:
    loaded_rfr = pickle.load(file)

test_predictions_clf = loaded_rfr.predict(test_data[selected_features_correlation])

print("Random Forest Regressor model R2 for test:", r2_score(test_data['rating'].values.ravel(), test_predictions_clf) * 100)